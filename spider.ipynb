{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlp import piop\n",
    "from pnlp import ptxt\n",
    "from pnlp import pmag\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from aip import AipNlp\n",
    "\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "from pypinyin import pinyin, lazy_pinyin, Style\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.lagou.com/zhaopin/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_gw = piop.read_yml('./data/category/category.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_url = piop.read_yml('./data/category/category_url_jointag.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlist_path = \"./data/position_urlist/\"\n",
    "html_path = \"./data/html/\"\n",
    "out_path = (\"./data/extract/\")\n",
    "seg_path = \"./data/segpos/\"\n",
    "model_path = \"./model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for key, value in cate_gw.items():\n",
    "    for key2, value2 in value.items():\n",
    "        tmp = []\n",
    "        for item in value2.split(\";\"):\n",
    "            pylist = lazy_pinyin(item)\n",
    "            url = BASE_URL + \"\".join(pylist) + \"/\"\n",
    "            tmp.append(\"\".join(pylist))\n",
    "            #print(key2, url)\n",
    "        res.append(\";\".join(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resurl = []\n",
    "n = 0\n",
    "for k,v in cate_url.items():\n",
    "    for k2, v2 in v.items():\n",
    "        lenth = (len(v2.split(\";\")))\n",
    "        resurl.append((k2,lenth))\n",
    "        n += lenth\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按类别获取所有职位链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urlist(title):\n",
    "    res = []\n",
    "    gangwei_base_url = BASE_URL + title + \"/\"\n",
    "#     arr = [w for w in range(1, 31)]\n",
    "    for i in range(1, 31):\n",
    "#         choose = random.randint(0, len(arr)-1)\n",
    "#         i = arr.pop(choose)\n",
    "        url = gangwei_base_url + str(i) + \"/?filterOption=2\"\n",
    "        page_urlist = get_pageurlist(url)\n",
    "        if len(page_urlist) > 0:\n",
    "            res.extend(page_urlist)\n",
    "        else:\n",
    "            break\n",
    "        sec = random.randint(1, 3)\n",
    "        time.sleep(sec)\n",
    "        \n",
    "        # 每 10 个重启一次\n",
    "        if i % 10 == 0:\n",
    "            driver.close()\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.maximize_window()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pageurlist(url):\n",
    "    position_links = []\n",
    "    driver.get(url)\n",
    "    driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "    ps = driver.page_source\n",
    "    soup = BeautifulSoup(ps)\n",
    "    is404 = soup.find(\"p\", {\"class\": \"tip\"})\n",
    "    if is404:\n",
    "        print(url, is404.get_text())\n",
    "    else:\n",
    "        position_links = soup.find_all(\"a\", {\"class\": \"position_link\"})\n",
    "        position_links = [pl.get(\"href\") for pl in position_links]\n",
    "    return position_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# flat all categories\n",
    "all_items = []\n",
    "for cate, gangwei_item in cate_gw.items():\n",
    "    for gangwei, zhiwei in gangwei_item.items():\n",
    "        zhiwei_list = zhiwei.split(\";\")\n",
    "        zhiwei_title_list = cate_url[cate][gangwei].split(\";\")\n",
    "        for i, zw in enumerate(zhiwei_list):\n",
    "            all_items.append((cate, gangwei, zw, zhiwei_title_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in all_items:\n",
    "    \n",
    "    url_dict = {}\n",
    "\n",
    "    cate = item[0]\n",
    "    gangwei = item[1]\n",
    "    zw = item[2]\n",
    "    title = item[3]\n",
    "\n",
    "    urlist = get_urlist(title)\n",
    "    \n",
    "    # 不满 30 页的\n",
    "    if len(urlist) < 15*30:\n",
    "        print(cate, \"\\t\", gangwei, \"\\t\", zw, \"\\t\", len(urlist))\n",
    "\n",
    "    url_dict['position'] = cate + \"_\" + gangwei + \"_\" + zw + \"_\" + title\n",
    "    url_dict['urlist'] = urlist\n",
    "\n",
    "    out_fpath = os.path.join(urlist_path, url_dict['position'] + \".txt\")\n",
    "    iopipe.write_json(out_fpath, url_dict, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按类别下载所有职位页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_html(fpath, page_source):\n",
    "    with open(fpath, 'w') as f:\n",
    "        f.write(page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def download():\n",
    "    driver = webdriver.Chrome()\n",
    "    filelist = os.listdir(urlist_path)\n",
    "    for file in filelist:\n",
    "        if file[0] == '.':\n",
    "            continue\n",
    "        data = piop.read_json(urlist_path + file)\n",
    "        fpath = html_path + data['position']\n",
    "        urlist = data['urlist']\n",
    "        for i, url in enumerate(urlist):\n",
    "            fname = os.path.split(url)[-1]\n",
    "            if os.path.exists(fpath + \"/\" + fname):\n",
    "                continue\n",
    "\n",
    "            driver.get(url)\n",
    "            ps = driver.page_source\n",
    "            write_html(fpath + \"/\" + fname, ps)\n",
    "            time.sleep(random.randint(1, 3))\n",
    "\n",
    "            # 每 10 个重启一次\n",
    "            if i % 10 == 0:\n",
    "                driver.close()\n",
    "                driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "技术_后端开发_区块链_qukuailian.txt 430 435\n",
      "技术_企业软件_售前工程师_shouqiangongchengshi.txt 423 435\n",
      "职能_行政_助理_zhuli.txt 434 435\n",
      "技术_运维_运维开发工程师_yunweikaifagongchengshi.txt 420 435\n",
      "设计_视觉设计_原画师_yuanhuashi.txt 421 435\n",
      "技术_后端开发_Go_go.txt 417 435\n",
      "技术_运维_系统安全_xitonganquan.txt 425 435\n",
      "游戏_其他_电竞主持_dianjingzhuchi.txt 428 435\n",
      "技术_后端开发_Python_Python.txt 433 435\n",
      "运营_运营_运营专员_yunyingzhuanyuan.txt 433 435\n",
      "产品_产品设计师_无线产品设计师_wuxianchanpinshejishi.txt 92 99\n",
      "产品_产品经理_产品实习生_chanpinshixisheng.txt 413 424\n",
      "技术_后端开发_全栈工程师_quanzhangongchengshi.txt 422 435\n",
      "运营_运营_活动运营_huodongyunying.txt 434 435\n",
      "技术_硬件开发_电路设计_dianlusheji.txt 434 435\n",
      "产品_产品经理_产品经理_chanpinjingli.txt 433 435\n",
      "运营_运营_运营经理_yunyingjingli.txt 434 435\n",
      "职能_财务_财务_caiwu.txt 433 435\n",
      "技术_后端开发_C++_C++.txt 430 435\n",
      "技术_高端职位_高端技术职位其它_gaoduanjishuzhiweiqita.txt 431 435\n",
      "运营_运营_产品运营_chanpinyunying.txt 389 435\n",
      "产品_产品经理_数据产品经理_shujuchanpinjingli.txt 427 435\n",
      "设计_用户研究_数据分析师_shujufenxishi.txt 426 435\n",
      "产品_产品经理_产品助理_chanpinzhuli.txt 428 435\n"
     ]
    }
   ],
   "source": [
    "# 重复 url 检验\n",
    "filelist = os.listdir(urlist_path)\n",
    "for file in filelist:\n",
    "    if file[0] == '.':\n",
    "        continue\n",
    "    data = piop.read_json(urlist_path + file)\n",
    "    fpath = html_path + data['position']\n",
    "    urlist = data['urlist']\n",
    "    filenum = len([_ for _ in os.listdir(fpath) if _[0] != '.'])\n",
    "    urlnum = len(urlist)\n",
    "    if filenum != urlnum:\n",
    "        print(file, filenum, urlnum)\n",
    "    if filenum != len(set(urlist)):\n",
    "        print(\"wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 将重复的下载页面用正确完整的页面替代\n",
    "for pname in os.listdir(html_path):\n",
    "    if pname[0] == '.':\n",
    "        continue\n",
    "    for base_file in os.listdir(html_path+pname):\n",
    "        tmp1, tmp2 = [], []\n",
    "        for pname in os.listdir(html_path):\n",
    "            if pname[0] == '.':\n",
    "                continue\n",
    "            for file in os.listdir(html_path+pname):\n",
    "                if file == base_file:\n",
    "                    base = os.path.join(html_path, pname, file)\n",
    "                    size = os.path.getsize(base)\n",
    "                    tmp1.append(base)\n",
    "                    tmp2.append(size)\n",
    "        if len(set(tmp2)) == 1:\n",
    "            continue\n",
    "        maxindex = tmp2.index(max(tmp2))\n",
    "        maxfile = tmp1[maxindex]\n",
    "        tmp1.remove(maxfile)\n",
    "        for ofile in tmp1:\n",
    "#             if os.path.getsize(ofile) < 15000:\n",
    "            shutil.copyfile(maxfile, ofile)\n",
    "#         print(base_file, \"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 404 页面\n",
    "urlset = []\n",
    "n = 0\n",
    "cal = 0\n",
    "driver = webdriver.Chrome()\n",
    "filelist = os.listdir(urlist_path)\n",
    "for file in filelist:\n",
    "    if file[0] == '.':\n",
    "        continue\n",
    "    data = piop.read_json(urlist_path + file)\n",
    "    fpath = html_path + data['position']\n",
    "    urlist = list(set(data['urlist']))\n",
    "    for i, url in enumerate(urlist):\n",
    "        fname = os.path.split(url)[-1]\n",
    "#         if os.path.exists(fpath + \"/\" + fname):\n",
    "#             n += 1\n",
    "#             urlset.append(url)\n",
    "#             continue\n",
    "#         downps = piop.read_file(fpath + \"/\" + fname)\n",
    "#         soup = BeautifulSoup(downps)\n",
    "#         is404 = soup.find(\"h4\", {\"class\": \"error_msg\"})\n",
    "        size = os.path.getsize(fpath + \"/\" + fname)\n",
    "        if size == 207786:\n",
    "            cal += 1\n",
    "#             print(fpath + \"/\" + fname)\n",
    "#             print(url)\n",
    "#             print(url, is404.get_text())\n",
    "\n",
    "            driver.get(url)\n",
    "            ps = driver.page_source\n",
    "            write_html(fpath + \"/\" + fname, ps)\n",
    "            time.sleep(random.randint(1, 3))\n",
    "\n",
    "            # 每 10 个重启一次\n",
    "            if cal % 10 == 0:\n",
    "                driver.close()\n",
    "                driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 57905 个不重复页面\n",
    "# 共 92842 个页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pool()\n",
    "for i in range(2):\n",
    "    p.apply_async(download, args=(i, ))\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按类别解析所有职位页面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92842 57905\n",
      "92842 57905\n"
     ]
    }
   ],
   "source": [
    "# 页面检查\n",
    "allist = []\n",
    "filelist = os.listdir(urlist_path)\n",
    "for file in filelist:\n",
    "    if file[0] == '.':\n",
    "        continue\n",
    "    data = piop.read_json(urlist_path + file)\n",
    "    fpath = html_path + data['position']\n",
    "    urlist = list(set(data['urlist']))\n",
    "    allist.extend(urlist)\n",
    "print(len(allist), len(set(allist)))\n",
    "\n",
    "allfile = []\n",
    "for pname in os.listdir(html_path):\n",
    "    if pname[0] == '.':\n",
    "        continue\n",
    "    for base_file in os.listdir(html_path+pname):\n",
    "        allfile.append(base_file)\n",
    "print(len(allfile), len(set(allfile)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出职位职责和职位要求的说明\n",
    "res = dict()\n",
    "tmp = dict()\n",
    "for pname in os.listdir(html_path):\n",
    "    if pname[0] == '.':\n",
    "        continue\n",
    "    for base_file in os.listdir(html_path+pname):\n",
    "        if base_file in tmp:\n",
    "            continue\n",
    "        tmp[base_file] = \"\"\n",
    "        html_file = os.path.join(html_path, pname, base_file)\n",
    "        raw_html = piop.read_file(html_file)\n",
    "        soup = BeautifulSoup(raw_html)\n",
    "        try:\n",
    "            jobdetail = soup.find(\"div\", {\"class\": \"job-detail\"}).get_text()\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        for item in re.compile(r\"\\n+\").split(jobdetail):\n",
    "            if re.compile(r'[\\da-zA-Z]+').search(item) or len(item) == 0 or len(item) > 8:\n",
    "                continue\n",
    "            item = \"||\".join(ptxt.Text(item, 'chi').extract.mats)\n",
    "            if item not in res:\n",
    "                res[item] = 1\n",
    "            else:\n",
    "                res[item] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4781"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_res = sorted(res.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/HaoShaochun/Desktop/job_description_mark_sorted_dict.txt\", 'w') as f:\n",
    "    for item in sorted_res:\n",
    "        item = item[0] + \"||\" + str(item[1])\n",
    "        f.write(item+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTIMEDICT = {\n",
    "    '1天前': \"2019-05-11\",\n",
    "    \"2天前\": \"2019-05-10\", \n",
    "    \"3天前\": \"2019-05-09\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_DESC_MARK_DICT = piop.read_lines(\n",
    "    \"/Users/HaoShaochun/Desktop/job_description_mark_sorted_dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pname in os.listdir(html_path):\n",
    "    if pname[0] == '.':\n",
    "        continue\n",
    "    print(\"=\"*20, pname, \"BEGIN\", \"=\"*20)\n",
    "    out_file = os.path.join(out_path, pname + \".txt\")\n",
    "    if os.path.exists(out_file):\n",
    "        continue\n",
    "    res = []\n",
    "    for base_file in os.listdir(html_path+pname):\n",
    "        html_file = os.path.join(html_path, pname, base_file)\n",
    "        try:\n",
    "            item = get_item(html_file)\n",
    "            res.append(item)\n",
    "        except Exception as e:\n",
    "            print(html_file, e)\n",
    "            continue\n",
    "    piop.write_json(out_file, res, indent=4, ensure_ascii=False)\n",
    "    print(\"=\"*20, pname, \"DONE\", \"=\"*20 + \"\\n\\n\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_duty_require(jobdetail):\n",
    "    res = {}\n",
    "    tmp = []\n",
    "    for item in re.compile(r'\\n+').split(jobdetail):\n",
    "        item_chi = \"\".join(ptxt.Text(item, 'chi').extract.mats)\n",
    "        if len(item_chi) == 0 or len(item_chi) > 8:\n",
    "            continue\n",
    "        for mark in JOB_DESC_MARK_DICT:\n",
    "            if mark in item:\n",
    "                tmp.append(mark)\n",
    "    tmp = sorted(tmp, key=lambda x: len(x), reverse=True)[:2]\n",
    "    rex  = \"|\".join(tmp)\n",
    "    reg = re.compile(rf'{rex}')\n",
    "    reslist = reg.split(jobdetail)\n",
    "    reslist = [_ for _ in reslist if len(ptxt.Text(_, 'chi').extract.mats) > 1]\n",
    "    return reslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(html_file):\n",
    "    \n",
    "    item = {}\n",
    "    raw_html = piop.read_file(html_file)\n",
    "    soup = BeautifulSoup(raw_html)\n",
    "\n",
    "    content = soup.find(\"div\", {\"class\": \"position-content-l\"})\n",
    "\n",
    "\n",
    "    company = content.find(\"div\", {\"class\": \"company\"}).get_text()\n",
    "    jobname = content.find(\"span\", {\"class\": \"name\"}).get_text()\n",
    "    salary = content.find(\"span\", {\"class\": \"salary\"}).get_text()\n",
    "    labels = content.find(\"ul\", {\"class\": \"position-label clearfix\"}\n",
    "                         ).get_text().replace(\"\\n\", \",\")[1:-1]\n",
    "    request = content.find(\"p\").get_text().split(\"\\n/\")[-1].replace(\" /\\n\", \",\")[:-1]\n",
    "    dtime = content.find(\"p\", {\"class\": \"publish_time\"}).get_text().split(\"\\xa0\")[0]\n",
    "    if \":\" in dtime:\n",
    "        dtime = \"2019-05-12\"\n",
    "    elif dtime in DTIMEDICT:\n",
    "        dtime = DTIMEDICT[dtime]\n",
    "    else:\n",
    "        dtime = dtime\n",
    "\n",
    "    jobadv = soup.find(\"dd\", {\"class\": \"job-advantage\"}).get_text().replace(\"\\n\", \"\")\n",
    "\n",
    "    jobdetail = soup.find(\"div\", {\"class\": \"job-detail\"}).get_text()\n",
    "    \n",
    "    duty_require_list = split_duty_require(jobdetail)\n",
    "    if len(duty_require_list) != 2:\n",
    "        duty, require = \"\", \"\"\n",
    "    else:\n",
    "        duty, require = duty_require_list\n",
    "        duty = ptxt.Text(duty, 'whi').clean\n",
    "        require = ptxt.Text(require, 'whi').clean\n",
    "    \n",
    "    item['category'] = \"_\".join(html_file.split(\"/\")[-2].split(\"_\")[:-1])\n",
    "    item['pname'] = html_file\n",
    "    item['company'] = company\n",
    "    item['jobname'] = jobname\n",
    "    item['salary'] = salary\n",
    "    item['labels'] = labels\n",
    "    item['request'] = request\n",
    "    item['jobadv'] = jobadv\n",
    "    item['duty'] = \"职位职责：\" + duty\n",
    "    item['require'] = \"职位要求：\" + require\n",
    "    item['jobdetail'] = jobdetail\n",
    "    item['dtime'] = dtime\n",
    "    \n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = \"16302011\"\n",
    "AK = \"jDOKyp4qpzeXFHSq91h7K0AG\"\n",
    "SK = \"70l8NbS35BG8NuiKhbzEQDBVWdX0HysS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AipNlp(ID, AK, SK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pmag.MagicDict()\n",
    "for file in os.listdir(out_path):\n",
    "    if file[0] == '.':\n",
    "        continue\n",
    "    fname = os.path.join(out_path, file)\n",
    "    outname = os.path.join(seg_path, file)\n",
    "    if os.path.exists(outname):\n",
    "        continue\n",
    "    data = dict()\n",
    "    cate, fduty, frequire, fdtimes = get_cate_res(fname)\n",
    "    data['cate'] = cate\n",
    "    data['duty'] = fduty\n",
    "    data['require'] = frequire\n",
    "    data['dtimes'] = fdtimes\n",
    "    piop.write_json(outname, data, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cate_res(cate_extract_file):\n",
    "    cate_item = piop.read_json(cate_extract_file)\n",
    "    dtimes = []\n",
    "    require = []\n",
    "    duty = []\n",
    "    for item in cate_item:\n",
    "        dtimes.append(item['dtime'])\n",
    "        if len(item['require']) < 10:\n",
    "            continue\n",
    "        try:\n",
    "            item_require = segpos(item['require'])\n",
    "            require.extend(item_require)\n",
    "        except Exception as e:\n",
    "            print(\"GET ITEM ERROR.\", e)\n",
    "            continue\n",
    "        try:\n",
    "            item_duty = segpos(item['duty'])\n",
    "            duty.extend(item_duty)\n",
    "        except Exception as e:\n",
    "            print(\"GET ITEM ERROR.\", e)\n",
    "            continue\n",
    "    try:\n",
    "        cate = item['category']\n",
    "    except Exception as e:\n",
    "        cate = \"\"\n",
    "    return cate, duty, require, dtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segpos(text):\n",
    "    res = []\n",
    "    try:\n",
    "        bd_resp = client.lexer(text)\n",
    "    except Exception as e:\n",
    "        print(\"BaiDu Error:\", e)\n",
    "    for item in bd_resp['items']:\n",
    "        pos = item['pos']\n",
    "        w = item['item']\n",
    "        res.append((w, pos))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2ngrams(lst, n, exact=True):\n",
    "    \"\"\" Convert list into character ngrams. \"\"\"\n",
    "    return [\"||\".join(lst[i:i+n]) for i in range(len(lst)-(n-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_in_list(lst1, lst2):\n",
    "    res = []\n",
    "    for item in lst1:\n",
    "        if item in lst2:\n",
    "            continue\n",
    "        else:\n",
    "            res.append(item)\n",
    "    if len(res) == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atleast_one_in_list(lst1, lst2):\n",
    "    res = []\n",
    "    for item in lst1:\n",
    "        if item in lst2:\n",
    "            res.append(item)\n",
    "    if len(res) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_common(wp_list, ignore, need):\n",
    "    res = []\n",
    "    for item in list2ngrams([w+\"||\"+p for (w,p) in wp_list], 2):\n",
    "        tmp = item.split(\"||\")\n",
    "        ww = [tmp[0], tmp[2]]\n",
    "        if atleast_one_in_list(ww, ignore):\n",
    "            continue\n",
    "        pp = [tmp[1], tmp[3]]\n",
    "        if every_in_list(pp, need):\n",
    "            res.append((\"\".join(ww))) #  + \" \" + \" \".join(pp)\n",
    "    \n",
    "#     return [(wp, f) for (wp, f) in Counter(res).most_common() if f >= 5]\n",
    "    return Counter(res).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_require(duty, require):\n",
    "    res = []\n",
    "    duty_wlist = [w for w,f in duty]\n",
    "    for item in require:\n",
    "        if item[0] in duty_wlist:\n",
    "            continue\n",
    "        else:\n",
    "            res.append(item)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_need_item(wf_list):\n",
    "    res = pmag.MagicDict()\n",
    "    n = sum([f for (w,f) in wf_list])\n",
    "    for w,f in wf_list:\n",
    "        res[w]['freq'] = f\n",
    "        res[w]['prob'] = f/n\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtime_item(dtime_list):\n",
    "    res = {}\n",
    "    count = len(dtime_list)\n",
    "    if count == 0:\n",
    "        res['count'] = 0\n",
    "        res['continuous_freq'] = 0\n",
    "        res['interval_freq'] = 0\n",
    "        res['publish_freq'] = 0\n",
    "    else:\n",
    "        sorted_dt = sorted(dtime_list)\n",
    "        begin = datetime.datetime.strptime(sorted_dt[0], \"%Y-%m-%d\")\n",
    "        end = datetime.datetime.strptime(sorted_dt[-1], \"%Y-%m-%d\")\n",
    "        res['count'] = count\n",
    "        res['continuous_freq'] = count/((end-begin).days+1)\n",
    "        res['interval_freq'] = count/len(set(dtime_list))\n",
    "        res['publish_freq'] = len(set(dtime_list))/((end-begin).days+1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE = piop.read_lines('./model/stop_words.txt')\n",
    "NEEDPOS = ['n', 'nz', 'vn', 'a', 'an']\n",
    "seg_files = sorted(os.listdir(seg_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pmag.MagicDict()\n",
    "for cate, _post in cate_gw.items():\n",
    "    duty_list, require_list, dtimes_list = [], [], []\n",
    "    tmp = pmag.MagicDict()\n",
    "    for post, job in _post.items():\n",
    "        tmp_duty_list, tmp_require_list, tmp_dtimes_list = [], [], []\n",
    "        tag = cate+\"_\"+post\n",
    "        for file in seg_files:\n",
    "            if tag in file:\n",
    "                fname = os.path.join(seg_path, file)\n",
    "                cate_data = piop.read_json(fname)\n",
    "                tmp_duty_list.extend(cate_data['duty'])\n",
    "                tmp_require_list.extend(cate_data['require'])\n",
    "                tmp_dtimes_list.extend(cate_data['dtimes'])\n",
    "        \n",
    "        tmp_duty = get_common(tmp_duty_list, IGNORE, NEEDPOS)\n",
    "        tmp_require = filter_require(tmp_duty, \n",
    "                                     get_common(tmp_require_list, IGNORE, NEEDPOS))\n",
    "        \n",
    "        tmp[post]['duty'] = get_need_item(tmp_duty)\n",
    "        tmp[post]['require'] = get_need_item(tmp_require)\n",
    "        tmp[post]['demand'] = get_dtime_item(tmp_dtimes_list)\n",
    "        \n",
    "        duty_list.extend(tmp_duty_list)\n",
    "        require_list.extend(tmp_require_list)\n",
    "        dtimes_list.extend(tmp_dtimes_list)\n",
    "    \n",
    "    duty = get_common(duty_list, IGNORE, NEEDPOS)\n",
    "    require = filter_require(duty, get_common(require_list, IGNORE, NEEDPOS))\n",
    "\n",
    "    res[cate]['duty'] = get_need_item(duty)\n",
    "    res[cate]['require'] = get_need_item(require)\n",
    "    res[cate]['demand'] = get_dtime_item(dtimes_list)\n",
    "    res[cate]['posts'] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "piop.write_json(os.path.join(model_path, \"model.txt\"), res, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = piop.read_json(os.path.join(model_path, \"model.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csfs = []\n",
    "ilfs = []\n",
    "for cate, _others in MODEL.items():\n",
    "    for post, others in _others['posts'].items():\n",
    "        item = MODEL[cate]['posts'][post]['demand']\n",
    "        csf = item.get('continuous_freq',0)\n",
    "        ilf = item.get('interval_freq',0)\n",
    "        csfs.append(csf)\n",
    "        ilfs.append(ilf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 39)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(csfs), len(ilfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1814, 1947)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(sum(csfs)), math.ceil(sum(ilfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chosen(chosen_wlist, wc_dict):\n",
    "    chosen = []\n",
    "    for w in chosen_wlist:\n",
    "        c = wc_dict[w]\n",
    "        chosen.append((w, c))\n",
    "    return chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = \"http://localhost:8003/api/generate\"\n",
    "anay = \"http://localhost:8003/api/choose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data = requests.post(init).json()\n",
    "like = dict(init_data['like'])\n",
    "cando = dict(init_data['cando'])\n",
    "text = (\"喜欢：\" + \",\".join([*like]) + \"\\n\\n\" + \"满足：\" + \",\".join([*cando]))\n",
    "with open(\"/Users/HaoShaochun/Desktop/pick.txt\", \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "picked = piop.read_file(\"/Users/HaoShaochun/Desktop/pick.txt\")\n",
    "tmp = picked.split(\"\\n\")\n",
    "plike = tmp[0][4:].split(',')\n",
    "pcando = tmp[-1][4:].split(',')\n",
    "chosen_like = get_chosen(plike, like)\n",
    "chosen_cando = get_chosen(pcando, cando)\n",
    "anay_res = requests.post(anay, json={\"user_chosen_dict\": \n",
    "                                     {\"like\": chosen_like, \"cando\": chosen_cando}}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['技术-后端开发', 100],\n",
       " ['设计-视觉设计', 94],\n",
       " ['销售-销售', 84],\n",
       " ['技术-运维', 74],\n",
       " ['技术-硬件开发', 73]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anay_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_requirelist(text):\n",
    "    res = []\n",
    "    pex= re.compile(r'''\n",
    "                    ([\\w+-]+n)+\n",
    "                    ''', re.UNICODE | re.VERBOSE)\n",
    "    cleaned_wt = clean_wt(text)\n",
    "    for item in \"\".join(cleaned_wt).split('w'):\n",
    "        for iitem in pex.finditer(item):\n",
    "            w = iitem.group()\n",
    "            res.append(w)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dutylist(text):\n",
    "    res = []\n",
    "    pex = re.compile(r'''\n",
    "                    [\\u4e00-\\u9fa5]+a[\\u4e00-\\u9fa5]+n\n",
    "                    |\n",
    "                    [\\u4e00-\\u9fa5]+n[\\u4e00-\\u9fa5]+v\n",
    "                    |\n",
    "                    [\\u4e00-\\u9fa5]+v[\\u4e00-\\u9fa5]+n\n",
    "                    ''', re.UNICODE | re.VERBOSE)\n",
    "    wt = segtag(text)\n",
    "    for item in \"\".join(wt).split('w'):\n",
    "        for iitem in pex.finditer(item):\n",
    "            w = iitem.group()\n",
    "            w = \"\".join(ptxt.Text(w, 'chi').extract.mats)\n",
    "            if w in IGNORE:\n",
    "                continue\n",
    "            res.append(w)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
